{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and prediction data\n",
    "training_data = TrainingData([\"./filteredtweets/hurricane0.json\", \"./filteredtweets/hurricane1.json\"], [\"./filteredtweets/weinstein0.json\"], train_frac=0.1)\n",
    "prediction_data = read_json([\"./filteredtweets/noMatch0.json\", \"./filteredtweets/noMatch1.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Multinomial Naive Bayes Classifier\n",
    "nb = MNBClassifier(training_data, prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict our prediction data\n",
    "nb.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on training data:  0.9931223077857171\n"
     ]
    }
   ],
   "source": [
    "# Print average train accuracy from kfold cross validation with k=5\n",
    "print(\"Mean accuracy on training data: \", nb.mean_accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training tweets:  66156\n",
      "Total training Hurricane Harvey:  51374\n",
      "Total training Harvey Weinstein:  14782\n",
      "Total prediction tweets:  1171276\n",
      "Total predicted as Hurricane Harvey:  971421\n",
      "Total predicted as Harvey Weinstein:  199855\n"
     ]
    }
   ],
   "source": [
    "print(\"Total training tweets: \", len(training_data.X))\n",
    "print(\"Total training Hurricane Harvey: \", len(training_data.hh_train))\n",
    "print(\"Total training Harvey Weinstein: \", len(training_data.hw_train))\n",
    "print(\"Total prediction tweets: \", len(nb.prediction_data))\n",
    "print(\"Total predicted as Hurricane Harvey: \", len(nb.hhind))\n",
    "print(\"Total predicted as Harvey Weinstein: \", len(nb.hwind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33431 33432 33433 33434 33435 33436 33438 33439 33440 33443 33444 33446\n",
      " 33447 33449 33450 33451 33453 33454 33455 33456 33457 33458 33460 33463\n",
      " 33465 33468 33475 33476 33480 33482 33483 33484 33485 33488 33489 33490\n",
      " 33492 33494 33495 33497 33498 33499 33500 33504 33506 33507 33508 33510\n",
      " 33512 33514 33515 33517 33519 33520 33521 33522 33523 33524 33525 33526\n",
      " 33527 33528 33529 33530 33535 33537 33538 33539 33544 33545 33546 33549\n",
      " 33550 33551 33552 33554 33555 33556 33557 33558 33559 33560 33561 33562\n",
      " 33563 33564 33565 33567 33568 33569 33570 33571 33572 33573 33575 33576\n",
      " 33577 33578 33580 33581]\n",
      "[1070376 1070377 1070378 1070379 1070380 1070381 1070382 1070383 1070384\n",
      " 1070385 1070386 1070387 1070388 1070389 1070390 1070391 1070392 1070393\n",
      " 1070394 1070395 1070396 1070397 1070398 1070399 1070400 1070401 1070402\n",
      " 1070403 1070404 1070405 1070406 1070407 1070408 1070409 1070410 1070411\n",
      " 1070412 1070413 1070414 1070415 1070416 1070417 1070418 1070419 1070420\n",
      " 1070421 1070422 1070423 1070424 1070425 1070426 1070427 1070428 1070429\n",
      " 1070430 1070431 1070432 1070433 1070434 1070435 1070436 1070437 1070438\n",
      " 1070439 1070440 1070441 1070442 1070443 1070444 1070445 1070446 1070447\n",
      " 1070448 1070449 1070450 1070451 1070452 1070453 1070454 1070455 1070456\n",
      " 1070457 1070458 1070459 1070460 1070461 1070462 1070463 1070464 1070465\n",
      " 1070466 1070467 1070468 1070469 1070470 1070471 1070472 1070473 1070474\n",
      " 1070475]\n",
      "RT Taiwan Goes All In Pledges 800K Dollars To Harvey Storm Relief hashtagharveyrelief hashtagharveyflood hashtagR\n",
      "RT did y all know gwyneth told brad about what harvey weinstein did to her and brad refused to ever work with him and\n"
     ]
    }
   ],
   "source": [
    "print(nb.hhind[30000:30100])\n",
    "print(nb.hwind[100000:100100])\n",
    "print(nb.prediction_data[35807])\n",
    "print(nb.prediction_data[1070460])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Hurricane Harvey features:  ['hashtagtexassearchandrescue' 'hashtaghelpandhopeforhouston'\n",
      " 'hashtaghelpforharvey' 'hashtaghelpforhouston' 'hashtaghelpharvey'\n",
      " 'hashtaghelphouston' 'hashtaghelpincrisis' 'hashtaghelping'\n",
      " 'hashtaghelpacripple' 'hashtaghelpinghand']\n",
      "Top Harvey Weinstein features:  ['men' 'trump' 'rt' 'spacey' 'kevin' 'did' 'brad' 'harvey' 'weinstein'\n",
      " 'fuck']\n"
     ]
    }
   ],
   "source": [
    "# Top words for each classifier\n",
    "tophh, tophw = nb.top_features(n=10)\n",
    "print(\"Top Hurricane Harvey features: \", tophh)\n",
    "print(\"Top Harvey Weinstein features: \", tophw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, KFold\n",
    "import re\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(files):\n",
    "    data = []\n",
    "    for file in files:\n",
    "        with open(file) as json_data:\n",
    "            data = data + [' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", tweet[\"text\"].replace(\"#\", \"hashtag\")).split()) for tweet in json.load(json_data)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNBClassifier(object):\n",
    "    def __init__(self, training_data, prediction_data):\n",
    "        self.training_data = training_data\n",
    "        self.prediction_data = prediction_data+training_data.hh_test+training_data.hw_test\n",
    "        self.nb = None\n",
    "        self.traincv = None\n",
    "        self.X_train_tfidf = None\n",
    "        self.hhind = None\n",
    "        self.hwind = None\n",
    "\n",
    "    def train(self):\n",
    "        self.nb = MultinomialNB()\n",
    "        self.traincv = CountVectorizer(stop_words='english')\n",
    "        X_train_counts = self.traincv.fit_transform(self.training_data.X)\n",
    "        self.X_train_tfidf = TfidfTransformer().fit_transform(X_train_counts)\n",
    "        self.nb.fit(self.X_train_tfidf, self.training_data.y)\n",
    "    \n",
    "    def mean_accuracy(self):\n",
    "        if self.nb is None:\n",
    "            self.train()\n",
    "        nbValidation = Cross_Validator(estimator=self.nb, X=self.X_train_tfidf, y=self.training_data.y)\n",
    "        nbValidation.run()\n",
    "        return nbValidation.get_mean_score()\n",
    "        \n",
    "    def top_features(self, n=10):\n",
    "        if self.nb is None:\n",
    "            self.train() \n",
    "        feature_names = np.asarray(self.traincv.get_feature_names())\n",
    "        bottom = np.argsort(self.nb.coef_[0])[:n]\n",
    "        top = np.argsort(self.nb.coef_[0])[-n:]\n",
    "\n",
    "        return feature_names[bottom], feature_names[top]\n",
    "    \n",
    "    def predict(self):\n",
    "        if self.nb is None:\n",
    "            self.train()\n",
    "        cv = CountVectorizer(stop_words='english', vocabulary=self.traincv.vocabulary_)\n",
    "        predict_counts = cv.transform(self.prediction_data)\n",
    "        predict_tfidf = TfidfTransformer().fit_transform(predict_counts)\n",
    "        yhat = self.nb.predict(predict_tfidf)\n",
    "        self.hhind = np.where(yhat == 0)[0]\n",
    "        self.hwind = np.where(yhat == 1)[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData(object):\n",
    "    def __init__(self, hurricane_filepath, weinstein_filepath, train_frac=1):\n",
    "        self.train_frac = train_frac\n",
    "        self.hh_train, self.hh_test = self.load_data(hurricane_filepath)\n",
    "        self.hw_train, self.hw_test = self.load_data(weinstein_filepath)\n",
    "        self.X, self.y = self.get_training_data()\n",
    "        \n",
    "    def load_data(self, file):\n",
    "        raw = read_json(file)\n",
    "        return raw[:int(len(raw)*self.train_frac)], raw[int(len(raw)*self.train_frac):len(raw)]\n",
    "        \n",
    "    def get_training_data(self):\n",
    "        return np.array(self.hh_train + self.hw_train), np.array(([0]*len(self.hh_train)) + ([1]*len(self.hw_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Validator(object):\n",
    "    def __init__(self, estimator, X, y, folds=5, trials=1):\n",
    "        self.estimator = estimator\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.folds = folds\n",
    "        self.scores = None\n",
    "        self.trials = trials\n",
    "        \n",
    "    def run(self):\n",
    "        all_test_scores = []\n",
    "\n",
    "        for ii in range(self.trials):\n",
    "            scores = cross_validate(self.estimator, self.X, self.y, cv=StratifiedKFold(n_splits=self.folds, shuffle=True, random_state=None), scoring=\"accuracy\")\n",
    "            all_test_scores = all_test_scores + list(scores[\"test_score\"])\n",
    "        self.scores = all_test_scores\n",
    "        \n",
    "    def get_mean_score(self):\n",
    "        if self.scores is None: self.run()\n",
    "        return np.mean(self.scores)\n",
    "    \n",
    "    def print_scores(self):\n",
    "        if self.scores is None: self.run()\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,4))\n",
    "        pd.DataFrame(self.scores).hist(ax=ax, color=mycolors[\"blue\"], edgecolor=\"white\")\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.set_title(\"Bootstrapped Cross-Val Accuracies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
