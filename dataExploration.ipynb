{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You must have data saved locally to run this file. Data is too large to save to git repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "tweet object: https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object  \n",
    "\n",
    "### Hurricane dates\n",
    "lands on Texas = August 26, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = []\n",
    "for line in open('data/tweets_1.json','r'):\n",
    "    tweet = json.loads(line)\n",
    "    dates.append( tweet['created_at']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "for line in open('data/tweets_1.json','r'):\n",
    "    tweet = json.loads(line)\n",
    "    text.append( tweet['text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "harvey_count = 0\n",
    "harvey_index = []\n",
    "for ii, tweet in enumerate(text):\n",
    "    if 'harvey' in tweet:\n",
    "        harvey_count += 1\n",
    "        harvey_index.append(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64291\n",
      "@xamilass Nao tem shipp maior que mike ross e harvey specter\n"
     ]
    }
   ],
   "source": [
    "print(harvey_count)\n",
    "print(text[harvey_index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est√° super grueso #Houston #HoustonStrong #HoustonFlood visto desde un #drone #drones https://t.co/VfqWL5ZRBi\n"
     ]
    }
   ],
   "source": [
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates.sort(key=lambda r:r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 31 00:00:20 +0000 2017 Sun Nov 12 17:11:30 +0000 2017\n"
     ]
    }
   ],
   "source": [
    "print (dates[0], dates[(len(dates)-1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-eba8fda59562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'harvey'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mgeog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetTweetData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-4fa26861bb8d>\u001b[0m in \u001b[0;36mgetTweetData\u001b[1;34m(d)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetTweetData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'expanded_url'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'urls'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'coordinates'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'coordinates'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "geog = []\n",
    "for line in open('data/tweets_23.json','r'):\n",
    "    tweet = json.loads(line)\n",
    "    if 'harvey' in tweet['text']:\n",
    "        geog.append(getTweetData(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweet id, date, text, location and URLs included in tweet\n",
    "\n",
    "def getTweetData(d):\n",
    "    urls = [url['expanded_url'] for url in d['entities']['urls']]\n",
    "    return(d['id'],d['created_at'],d['text'],urls,d['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening json  0\n",
      "opening json  1\n",
      "opening json  2\n",
      "opening json  3\n",
      "opening json  4\n",
      "opening json  5\n",
      "opening json  6\n",
      "opening json  7\n",
      "opening json  8\n",
      "opening json  9\n",
      "opening json  10\n",
      "opening json  11\n",
      "opening json  12\n",
      "opening json  13\n",
      "opening json  14\n",
      "opening json  15\n",
      "opening json  16\n",
      "opening json  17\n",
      "opening json  18\n",
      "opening json  19\n",
      "opening json  20\n",
      "opening json  21\n",
      "opening json  22\n",
      "opening json  23\n"
     ]
    }
   ],
   "source": [
    "#open files one by one and make collection of tweets that include the word Harvey\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export = []\n",
    "    for i in range(24):\n",
    "        print(\"opening json \", i)\n",
    "        for line in open('data/tweets_' + str(i) + '.json','r'):\n",
    "            tweet = json.loads(line)\n",
    "            if 'harvey' in tweet['text']:\n",
    "                export.append(getTweetData(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1237666"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# back-ups, because, well it's me and it took a LONG time to filter the data\n",
    "export1=export\n",
    "export2=export\n",
    "export3=export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>903053412509425664</td>\n",
       "      <td>Thu Aug 31 00:34:46 +0000 2017</td>\n",
       "      <td>RT @Tedashii: Losing a loved one is a hard thi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>903055227833614339</td>\n",
       "      <td>Thu Aug 31 00:41:58 +0000 2017</td>\n",
       "      <td>RT @Jenni_77: Please RT\\n#MISSING last seen Ho...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>903061868100685825</td>\n",
       "      <td>Thu Aug 31 01:08:22 +0000 2017</td>\n",
       "      <td>RT @TheMercedesXXX: Please share! #harvey #Hou...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>903073788497190913</td>\n",
       "      <td>Thu Aug 31 01:55:44 +0000 2017</td>\n",
       "      <td>RT @JerryMarkon: The criticism of Trump's hand...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>903082750491668480</td>\n",
       "      <td>Thu Aug 31 02:31:20 +0000 2017</td>\n",
       "      <td>RT @Tedashii: Losing a loved one is a hard thi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                            date  \\\n",
       "0  903053412509425664  Thu Aug 31 00:34:46 +0000 2017   \n",
       "1  903055227833614339  Thu Aug 31 00:41:58 +0000 2017   \n",
       "2  903061868100685825  Thu Aug 31 01:08:22 +0000 2017   \n",
       "3  903073788497190913  Thu Aug 31 01:55:44 +0000 2017   \n",
       "4  903082750491668480  Thu Aug 31 02:31:20 +0000 2017   \n",
       "\n",
       "                                                text url location  \n",
       "0  RT @Tedashii: Losing a loved one is a hard thi...  []     None  \n",
       "1  RT @Jenni_77: Please RT\\n#MISSING last seen Ho...  []     None  \n",
       "2  RT @TheMercedesXXX: Please share! #harvey #Hou...  []     None  \n",
       "3  RT @JerryMarkon: The criticism of Trump's hand...  []     None  \n",
       "4  RT @Tedashii: Losing a loved one is a hard thi...  []     None  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert list to dataframe\n",
    "df1 = pd.DataFrame(np.array(export1))\n",
    "df1.columns = [\"id\", \"date\", \"text\", \"url\", \"location\"]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save as csv\n",
    "\n",
    "df1.to_csv('data/harvey2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
