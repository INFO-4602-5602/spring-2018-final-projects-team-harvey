{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNB:\n",
    "    def __init__(self, X_train, y_train, alpha=1.0):\n",
    "        \"\"\"\n",
    "        :param X_train: a list or ndarray of text strings to use as training data \n",
    "        :param y_train: an ndarray of true labels associated with the text data \n",
    "        :param alpha: the Laplace smoothing parameter \n",
    "        \"\"\"\n",
    "        \n",
    "        # store training data \n",
    "        self.X_train = X_train \n",
    "        self.y_train = y_train \n",
    "        \n",
    "        # store smoothing parameter\n",
    "        self.alpha = alpha \n",
    "        \n",
    "        # get number of classes \n",
    "        self.num_classes = len(set(y_train))\n",
    "        \n",
    "        # initialize vocab to feature map \n",
    "        self.vocab = dict() \n",
    "        \n",
    "        # initialize class counts \n",
    "        self.class_counts = np.zeros(self.num_classes, dtype=int)\n",
    "        \n",
    "        # track total docs\n",
    "        self.total_docs = 0\n",
    "        \n",
    "        # initialize feature counts (Note, will need to update this with the correct\n",
    "        # number of columns during the training process)\n",
    "        self.feature_counts = np.zeros((self.num_classes, 0), dtype=int)\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Learn the vocabularly, class_counts, and feature counts from the training data \n",
    "        \"\"\"\n",
    "        j = 0\n",
    "        for i in range(0, len(self.X_train)):\n",
    "            words = self.X_train[i].split()\n",
    "            for word in words:\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = j\n",
    "                    j += 1\n",
    "                \n",
    "            self.class_counts[self.y_train[i]] += 1\n",
    "            self.total_docs += 1\n",
    "        \n",
    "        # initialize feature counts \n",
    "        self.feature_counts = np.zeros((self.num_classes, len(self.vocab)), dtype=int)\n",
    "        \n",
    "        for i in range(0, len(self.X_train)):\n",
    "            words = self.X_train[i].split()\n",
    "            for word in words:\n",
    "                self.feature_counts[self.y_train[i]][self.vocab[word]] += 1                    \n",
    "                    \n",
    "    def predict_log_score(self, text_str):\n",
    "        \"\"\"\n",
    "        Get the log-probability score for each class\n",
    "        for a query string\n",
    "        \n",
    "        :param text_str: a single string of text to compute the log_score for \n",
    "        \"\"\"\n",
    "        class_scores = np.zeros(self.num_classes) \n",
    "        words = text_str.split()            \n",
    "        \n",
    "        for c in range(0, self.num_classes):\n",
    "            for word in words:\n",
    "                if word in self.vocab:\n",
    "                    class_scores[c] += np.log((self.feature_counts[c][self.vocab[word]]+self.alpha)/(np.sum(self.feature_counts[c])+(self.alpha*len(self.vocab))))\n",
    "            class_scores[c] += np.log((self.class_counts[c]+self.alpha)/(self.total_docs+(self.alpha*self.num_classes)))\n",
    "        \n",
    "        return class_scores\n",
    "        \n",
    "    \n",
    "    def predict(self, text_list):\n",
    "        \"\"\"\n",
    "        Predict the class of each example in text_list  \n",
    "        \n",
    "        :param text_list: a list or ndarray of text strings to make predictions on \n",
    "        \"\"\"\n",
    "                \n",
    "        yhat = np.zeros(len(text_list), dtype=int)\n",
    "        \n",
    "        for ii, x in enumerate(text_list):\n",
    "            class_scores = self.predict_log_score(x)\n",
    "            yhat[ii] = np.argmax(class_scores)\n",
    "        \n",
    "        return yhat \n",
    "        \n",
    "        \n",
    "    def accuracy(self, text_list, y_true):\n",
    "        \"\"\"\n",
    "        Make predictions on texts in text_list and compute accuracy relative to \n",
    "        true labels in y_true \n",
    "        \n",
    "        :param text_list: a list or ndarray of text strings to make predictions on \n",
    "        :param y_list: an ndarray of true labels associated with the text data \n",
    "        \"\"\"\n",
    "        yhat = self.predict(text_list)\n",
    "        return np.sum(yhat == y_true)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data, label it, and split into training and validation sets\n",
    "training_data = TrainingData(\"./filteredtweets/hurricane.json\", \"./filteredtweets/weinstein.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NB classifier using training data\n",
    "nb = TextNB(training_data.X_train, training_data.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "nb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995475429310134"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the accuracy of training data\n",
    "nb.accuracy(training_data.X_train, training_data.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9965354202331458"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the accuracy of validation data\n",
    "nb.accuracy(training_data.x_valid, training_data.y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "#load data to classify\n",
    "#predict labels on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData(object):\n",
    "    def __init__(self, hurricane_filepath, weinstein_filepath, train_frac=0.7):\n",
    "        self.train_frac = train_frac\n",
    "        self.hurricane = self.read_json(hurricane_filepath)\n",
    "        self.weinstein = self.read_json(weinstein_filepath)\n",
    "        X, y = self.get_training_data()\n",
    "        self.X_train, self.x_valid, self.y_train, self.y_valid = self.train_valid_split(X, y)\n",
    "        \n",
    "    def read_json(self, filepath):\n",
    "        with open(filepath) as json_data:\n",
    "            return json.load(json_data) \n",
    "        \n",
    "    def get_training_data(self):\n",
    "        return np.array([tweet[\"text\"] for tweet in self.hurricane] + [tweet[\"text\"] for tweet in self.weinstein]), np.array(([0]*len(self.hurricane)) + ([1]*len(self.weinstein)))\n",
    "        \n",
    "    def train_valid_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Randomly splits the data into training and validation sets \n",
    "\n",
    "        :param X: (n x p) ndarray of feature data \n",
    "        :param y: (n x 1) ndarray of labels/targets  \n",
    "        :param train_frac: float indicating fraction of data to train on \n",
    "        \"\"\"\n",
    "        \n",
    "        # Reference: https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "        X_concat_y = np.c_[X.reshape(len(X), -1), y.reshape(len(y), -1)]\n",
    "        np.random.shuffle(X_concat_y)\n",
    "        \n",
    "        # Now that it has been shuffled, break them back into seperate arrays\n",
    "        X = X_concat_y[:, :X.size//len(X)].reshape(X.shape)\n",
    "        y = X_concat_y[:, X.size//len(X):].reshape(y.shape)\n",
    "        \n",
    "        # Split based on tran_frac percentage\n",
    "        X_train = X[:int(len(X)*self.train_frac)]\n",
    "        X_valid = X[int(len(X)*self.train_frac):]\n",
    "        \n",
    "        y_train = y[:int(len(y)*self.train_frac)]\n",
    "        y_valid = y[int(len(y)*self.train_frac):]\n",
    "        \n",
    "        return np.array(X_train), np.array(X_valid), np.array(y_train).astype(int), np.array(y_valid).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
