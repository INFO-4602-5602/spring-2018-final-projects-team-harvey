{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training and prediction data\n",
    "training_data = TrainingData([\"./filteredtweets/hurricane0.json\", \"./filteredtweets/hurricane1.json\"], [\"./filteredtweets/weinstein0.json\"], train_frac=0.1)\n",
    "prediction_data = read_json([\"./filteredtweets/noMatch0.json\", \"./filteredtweets/noMatch1.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build Multinomial Naive Bayes Classifier\n",
    "nb = MNBClassifier(training_data, prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict our prediction data\n",
    "nb.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'coordinates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-866e62ef5b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_geojson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-f2b60fb049ca>\u001b[0m in \u001b[0;36mto_geojson\u001b[0;34m(self, hhloc, hwloc)\u001b[0m\n\u001b[1;32m     72\u001b[0m             geo_json_feature = {\n\u001b[1;32m     73\u001b[0m                 \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Feature\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0;34m\"geometry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coordinates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \"properties\": {\n\u001b[1;32m     76\u001b[0m                     \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'coordinates'"
     ]
    }
   ],
   "source": [
    "nb.to_geojson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, KFold\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_json(files):\n",
    "    data = []\n",
    "    for file in files:\n",
    "        with open(file) as json_data:\n",
    "            for tweet in json.load(json_data):\n",
    "                tweet[\"text\"] = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", tweet[\"text\"].replace(\"#\", \"hashtag\")).split())\n",
    "                data.append(tweet)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNBClassifier(object):\n",
    "    def __init__(self, training_data, prediction_data):\n",
    "        self.raw_train = np.array(training_data)\n",
    "        self.raw_test = np.array(prediction_data+training_data.hh_test+training_data.hw_test)\n",
    "        self.X_train, self.y_train = [tweet[\"text\"] for tweet in training_data.X], training_data.y\n",
    "        self.prediction_data = [tweet[\"text\"] for tweet in prediction_data+training_data.hh_test+training_data.hw_test]\n",
    "        self.nb = None\n",
    "        self.traincv = None\n",
    "        self.X_train_tfidf = None\n",
    "        self.hhind = None\n",
    "        self.hwind = None\n",
    "\n",
    "    def train(self):\n",
    "        self.nb = MultinomialNB()\n",
    "        self.traincv = CountVectorizer(stop_words='english')\n",
    "        X_train_counts = self.traincv.fit_transform(self.X_train)\n",
    "        self.X_train_tfidf = TfidfTransformer().fit_transform(X_train_counts)\n",
    "        self.nb.fit(self.X_train_tfidf, self.y_train)\n",
    "    \n",
    "    def mean_accuracy(self):\n",
    "        if self.nb is None:\n",
    "            self.train()\n",
    "        nbValidation = Cross_Validator(estimator=self.nb, X=self.X_train_tfidf, y=self.y_train)\n",
    "        nbValidation.run()\n",
    "        return nbValidation.get_mean_score()\n",
    "        \n",
    "    def top_features(self, n=10):\n",
    "        if self.nb is None:\n",
    "            self.train() \n",
    "        feature_names = np.asarray(self.traincv.get_feature_names())\n",
    "        bottom = np.argsort(self.nb.coef_[0])[:n]\n",
    "        top = np.argsort(self.nb.coef_[0])[-n:]\n",
    "\n",
    "        return feature_names[bottom], feature_names[top]\n",
    "    \n",
    "    def predict(self):\n",
    "        if self.nb is None:\n",
    "            self.train()\n",
    "        cv = CountVectorizer(stop_words='english', vocabulary=self.traincv.vocabulary_)\n",
    "        predict_counts = cv.transform(self.prediction_data)\n",
    "        predict_tfidf = TfidfTransformer().fit_transform(predict_counts)\n",
    "        yhat = self.nb.predict(predict_tfidf)\n",
    "        self.hhind = np.where(yhat == 0)[0]\n",
    "        self.hwind = np.where(yhat == 1)[0]\n",
    "        \n",
    "    def to_csv(self, hhloc=\"./classifiedtweets/hhtweets\", hwloc=\"./classifiedtweets/hwtweets\"):\n",
    "        hhtweets = self.raw_test[self.hhind]\n",
    "        hwtweets = self.raw_test[self.hwind]\n",
    "        \n",
    "        hhtweetsdate = pd.DataFrame([{\"text\": tweet[\"text\"], \"date\": tweet[\"date\"]} for tweet in hhtweets])\n",
    "        hhtweetsloc = pd.DataFrame([{\"text\": tweet[\"text\"], \"location\": tweet[\"location\"]} for tweet in hhtweets])\n",
    "        \n",
    "        hwtweetsdate = pd.DataFrame([{\"text\": tweet[\"text\"], \"date\": tweet[\"date\"]} for tweet in hwtweets])\n",
    "        hwtweetsloc = pd.DataFrame([{\"text\": tweet[\"text\"], \"location\": tweet[\"location\"]} for tweet in hwtweets])\n",
    "        \n",
    "        hhtweetsdate.to_csv(hhloc + 'date' + '.csv', index=False)\n",
    "        hhtweetsloc.to_csv(hhloc + 'location' + '.csv', index=False)\n",
    "        \n",
    "        hwtweetsdate.to_csv(hwloc + 'date' + '.csv', index=False)\n",
    "        hwtweetsloc.to_csv(hwloc + 'location' + '.csv', index=False)\n",
    "        \n",
    "    def to_geojson(self, hhloc=\"./classifiedtweets/hhtweets\", hwloc=\"./classifiedtweets/hwtweets\"):\n",
    "        hhtweets = self.raw_test[self.hhind]\n",
    "        hwtweets = self.raw_test[self.hwind]\n",
    "        \n",
    "        geo_data = {\n",
    "            \"type\" : \"FeatureCollection\",\n",
    "            \"features\": []\n",
    "        }\n",
    "        \n",
    "        for tweet in hhtweets [1:]:\n",
    "            geo_json_feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": tweet[''],\n",
    "                \"properties\": {\n",
    "                    \"text\": tweet['text'],\n",
    "                    \"created_at\": tweet['created_at']\n",
    "                }\n",
    "            }\n",
    "            geo_data['features'].append(geo_json_feature)\n",
    " \n",
    "        # Save geo data\n",
    "        with open('geo_data.json', 'w') as fout:\n",
    "            fout.write(json.dumps(geo_data, indent=4))\n",
    "    \n",
    "    def to_json(self, hhloc=\"./classifiedtweets/hhtweets\", hwloc=\"./classifiedtweets/hwtweets\"):\n",
    "        hhtweets = self.raw_test[self.hhind]\n",
    "        hwtweets = self.raw_test[self.hwind]\n",
    "        b = hhtweets.tolist() # nested lists with same data, indices\n",
    "        file_path = \"geo_list.json\" ## your path variable\n",
    "        json.dump(b, codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4) ### this saves the array in .json forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingData(object):\n",
    "    def __init__(self, hurricane_filepath, weinstein_filepath, train_frac=1):\n",
    "        self.train_frac = train_frac\n",
    "        self.hh_train, self.hh_test = self.load_data(hurricane_filepath)\n",
    "        self.hw_train, self.hw_test = self.load_data(weinstein_filepath)\n",
    "        self.X, self.y = self.get_training_data()\n",
    "        \n",
    "    def load_data(self, file):\n",
    "        raw = read_json(file)\n",
    "        return raw[:int(len(raw)*self.train_frac)], raw[int(len(raw)*self.train_frac):len(raw)]\n",
    "        \n",
    "    def get_training_data(self):\n",
    "        return np.array(self.hh_train + self.hw_train), np.array(([0]*len(self.hh_train)) + ([1]*len(self.hw_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Cross_Validator(object):\n",
    "    def __init__(self, estimator, X, y, folds=5, trials=1):\n",
    "        self.estimator = estimator\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.folds = folds\n",
    "        self.scores = None\n",
    "        self.trials = trials\n",
    "        \n",
    "    def run(self):\n",
    "        all_test_scores = []\n",
    "\n",
    "        for ii in range(self.trials):\n",
    "            scores = cross_validate(self.estimator, self.X, self.y, cv=StratifiedKFold(n_splits=self.folds, shuffle=True, random_state=None), scoring=\"accuracy\")\n",
    "            all_test_scores = all_test_scores + list(scores[\"test_score\"])\n",
    "        self.scores = all_test_scores\n",
    "        \n",
    "    def get_mean_score(self):\n",
    "        if self.scores is None: self.run()\n",
    "        return np.mean(self.scores)\n",
    "    \n",
    "    def print_scores(self):\n",
    "        if self.scores is None: self.run()\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,4))\n",
    "        pd.DataFrame(self.scores).hist(ax=ax, color=mycolors[\"blue\"], edgecolor=\"white\")\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.set_title(\"Bootstrapped Cross-Val Accuracies\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
